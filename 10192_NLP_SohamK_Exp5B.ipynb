{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y9c2g-5T8-tl",
        "outputId": "089bb14e-3d89-421e-ad8b-810b99c3888c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e5daf2d6aa164e6da6bdefc775899b10"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install nltk gensim scikit-learn matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 1: CUSTOM STOPWORD REMOVAL\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample text for stopword removal\n",
        "sample_text = \"\"\"\n",
        "Natural language processing is a fascinating field. It involves many techniques\n",
        "like tokenization, stemming, and classification. The results can be very useful\n",
        "for many applications in the real world.\n",
        "\"\"\"\n",
        "\n",
        "# Create custom stopword list (different from NLTK)\n",
        "custom_stopwords = {\n",
        "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
        "    'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
        "    'to', 'was', 'were', 'will', 'with', 'can', 'very', 'like', 'many'\n",
        "}\n",
        "\n",
        "def preprocess_text(text, remove_stopwords=True):\n",
        "    \"\"\"Preprocess text with optional stopword removal\"\"\"\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    words = text.split()\n",
        "\n",
        "    if remove_stopwords:\n",
        "        words = [word for word in words if word not in custom_stopwords]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Compare with and without stopwords\n",
        "print(\"TASK 1: Custom Stopword Removal\")\n",
        "print(\"=\"*50)\n",
        "words_with_stopwords = preprocess_text(sample_text, remove_stopwords=False)\n",
        "words_without_stopwords = preprocess_text(sample_text, remove_stopwords=True)\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(sample_text)\n",
        "print(f\"\\nWith stopwords ({len(words_with_stopwords)} words):\")\n",
        "print(\" \".join(words_with_stopwords))\n",
        "print(f\"\\nWithout stopwords ({len(words_without_stopwords)} words):\")\n",
        "print(\" \".join(words_without_stopwords))\n",
        "print(f\"Reduction: {len(words_with_stopwords) - len(words_without_stopwords)} words removed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiPJHNA69GAc",
        "outputId": "d23e8c14-dced-4086-f992-b34cd823ef86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TASK 1: Custom Stopword Removal\n",
            "==================================================\n",
            "Original text:\n",
            "\n",
            "Natural language processing is a fascinating field. It involves many techniques \n",
            "like tokenization, stemming, and classification. The results can be very useful \n",
            "for many applications in the real world.\n",
            "\n",
            "\n",
            "With stopwords (29 words):\n",
            "natural language processing is a fascinating field it involves many techniques like tokenization stemming and classification the results can be very useful for many applications in the real world\n",
            "\n",
            "Without stopwords (15 words):\n",
            "natural language processing fascinating field involves techniques tokenization stemming classification results useful applications real world\n",
            "Reduction: 14 words removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 2: VOCABULARY BUILDER - TERM-DOCUMENT MATRIX\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 2: Vocabulary Builder - Term-Document Matrix\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 10 short sentences for vocabulary building\n",
        "sentences = [\n",
        "    \"I love programming with Python\",\n",
        "    \"Machine learning is very interesting\",\n",
        "    \"Python is great for data science\",\n",
        "    \"I enjoy working with data\",\n",
        "    \"Machine learning uses Python frequently\",\n",
        "    \"Data science requires programming skills\",\n",
        "    \"Python programming is fun and useful\",\n",
        "    \"I love data analysis and visualization\",\n",
        "    \"Machine learning algorithms are powerful\",\n",
        "    \"Programming with Python is efficient\"\n",
        "]\n",
        "\n",
        "# Build term-document matrix\n",
        "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
        "term_doc_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Get feature names (vocabulary)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "td_matrix_df = pd.DataFrame(\n",
        "    term_doc_matrix.toarray(),\n",
        "    columns=vocab,\n",
        "    index=[f\"Doc_{i+1}\" for i in range(len(sentences))]\n",
        ")\n",
        "\n",
        "print(\"Term-Document Matrix:\")\n",
        "print(td_matrix_df)\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
        "print(f\"Total documents: {len(sentences)}\")\n",
        "\n",
        "# Word distribution analysis\n",
        "word_frequencies = np.sum(term_doc_matrix.toarray(), axis=0)\n",
        "word_freq_df = pd.DataFrame({\n",
        "    'Word': vocab,\n",
        "    'Frequency': word_frequencies\n",
        "}).sort_values('Frequency', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 most frequent words:\")\n",
        "print(word_freq_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMcH5-MV9srs",
        "outputId": "6baad93f-fd4f-42ae-c556-bbfc9b97b5a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 2: Vocabulary Builder - Term-Document Matrix\n",
            "==================================================\n",
            "Term-Document Matrix:\n",
            "        algorithms  analysis  data  efficient  enjoy  frequently  fun  great  \\\n",
            "Doc_1            0         0     0          0      0           0    0      0   \n",
            "Doc_2            0         0     0          0      0           0    0      0   \n",
            "Doc_3            0         0     1          0      0           0    0      1   \n",
            "Doc_4            0         0     1          0      1           0    0      0   \n",
            "Doc_5            0         0     0          0      0           1    0      0   \n",
            "Doc_6            0         0     1          0      0           0    0      0   \n",
            "Doc_7            0         0     0          0      0           0    1      0   \n",
            "Doc_8            0         1     1          0      0           0    0      0   \n",
            "Doc_9            1         0     0          0      0           0    0      0   \n",
            "Doc_10           0         0     0          1      0           0    0      0   \n",
            "\n",
            "        interesting  learning  ...  powerful  programming  python  requires  \\\n",
            "Doc_1             0         0  ...         0            1       1         0   \n",
            "Doc_2             1         1  ...         0            0       0         0   \n",
            "Doc_3             0         0  ...         0            0       1         0   \n",
            "Doc_4             0         0  ...         0            0       0         0   \n",
            "Doc_5             0         1  ...         0            0       1         0   \n",
            "Doc_6             0         0  ...         0            1       0         1   \n",
            "Doc_7             0         0  ...         0            1       1         0   \n",
            "Doc_8             0         0  ...         0            0       0         0   \n",
            "Doc_9             0         1  ...         1            0       0         0   \n",
            "Doc_10            0         0  ...         0            1       1         0   \n",
            "\n",
            "        science  skills  useful  uses  visualization  working  \n",
            "Doc_1         0       0       0     0              0        0  \n",
            "Doc_2         0       0       0     0              0        0  \n",
            "Doc_3         1       0       0     0              0        0  \n",
            "Doc_4         0       0       0     0              0        1  \n",
            "Doc_5         0       0       0     1              0        0  \n",
            "Doc_6         1       1       0     0              0        0  \n",
            "Doc_7         0       0       1     0              0        0  \n",
            "Doc_8         0       0       0     0              1        0  \n",
            "Doc_9         0       0       0     0              0        0  \n",
            "Doc_10        0       0       0     0              0        0  \n",
            "\n",
            "[10 rows x 22 columns]\n",
            "\n",
            "Vocabulary size: 22\n",
            "Total documents: 10\n",
            "\n",
            "Top 10 most frequent words:\n",
            "             Word  Frequency\n",
            "14         python          5\n",
            "13    programming          4\n",
            "2            data          4\n",
            "11        machine          3\n",
            "9        learning          3\n",
            "16        science          2\n",
            "10           love          2\n",
            "20  visualization          1\n",
            "19           uses          1\n",
            "18         useful          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 3: MINI SENTIMENT CLASSIFIER (NAIVE BAYES)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 3: Mini Sentiment Classifier (Naive Bayes)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dataset of positive and negative reviews\n",
        "positive_reviews = [\n",
        "    \"This movie is absolutely fantastic and amazing\",\n",
        "    \"I loved every moment of this wonderful film\",\n",
        "    \"Brilliant acting and excellent storyline\",\n",
        "    \"Outstanding performance by all actors\",\n",
        "    \"This is the best movie I have ever seen\",\n",
        "    \"Incredible cinematography and great direction\",\n",
        "    \"Perfect blend of action and emotion\",\n",
        "    \"Highly recommend this masterpiece\",\n",
        "    \"Exceptional quality and superb entertainment\",\n",
        "    \"Amazing experience and wonderful journey\"\n",
        "]\n",
        "\n",
        "negative_reviews = [\n",
        "    \"This movie is terrible and boring\",\n",
        "    \"Worst film I have ever watched\",\n",
        "    \"Poor acting and weak storyline\",\n",
        "    \"Completely disappointed with this movie\",\n",
        "    \"Waste of time and money\",\n",
        "    \"Horrible direction and bad script\",\n",
        "    \"Terrible performance by actors\",\n",
        "    \"Boring and predictable plot\",\n",
        "    \"Very disappointing and poorly made\",\n",
        "    \"Awful movie with no entertainment value\"\n",
        "]\n",
        "\n",
        "# Prepare training data\n",
        "X_train = positive_reviews + negative_reviews\n",
        "y_train = [1] * len(positive_reviews) + [0] * len(negative_reviews)  # 1 = positive, 0 = negative\n",
        "\n",
        "# Vectorize the text\n",
        "sentiment_vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
        "X_train_vec = sentiment_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Test on unseen sentences\n",
        "test_sentences = [\n",
        "    \"This is an excellent and wonderful movie\",\n",
        "    \"Terrible acting and boring plot\",\n",
        "    \"I really enjoyed this film\",\n",
        "    \"Complete waste of time\"\n",
        "]\n",
        "\n",
        "X_test_vec = sentiment_vectorizer.transform(test_sentences)\n",
        "predictions = nb_classifier.predict(X_test_vec)\n",
        "probabilities = nb_classifier.predict_proba(X_test_vec)\n",
        "\n",
        "print(\"Sentiment Classification Results:\")\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    sentiment = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
        "    confidence = max(probabilities[i]) * 100\n",
        "    print(f\"'{sentence}' → {sentiment} (confidence: {confidence:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0FzhrH-Ggb",
        "outputId": "4720054b-9457-44ce-dcca-037eed3077d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 3: Mini Sentiment Classifier (Naive Bayes)\n",
            "==================================================\n",
            "Sentiment Classification Results:\n",
            "'This is an excellent and wonderful movie' → Positive (confidence: 79.2%)\n",
            "'Terrible acting and boring plot' → Negative (confidence: 95.8%)\n",
            "'I really enjoyed this film' → Negative (confidence: 51.4%)\n",
            "'Complete waste of time' → Negative (confidence: 81.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 4: WORD2VEC MODEL TRAINING (Improved Version)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 4: Word2Vec Model Training (Improved Version)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------------\n",
        "# Initial toy dataset\n",
        "# -------------------------\n",
        "initial_sentences = [\n",
        "    [\"dog\", \"is\", \"a\", \"loyal\", \"pet\", \"animal\"],\n",
        "    [\"cat\", \"likes\", \"to\", \"sleep\", \"and\", \"play\"],\n",
        "    [\"dog\", \"loves\", \"playing\", \"fetch\", \"outside\"],\n",
        "    [\"cat\", \"enjoys\", \"hunting\", \"small\", \"mice\"],\n",
        "    [\"pet\", \"animals\", \"bring\", \"joy\", \"to\", \"families\"],\n",
        "    [\"dog\", \"barks\", \"when\", \"strangers\", \"approach\"],\n",
        "    [\"cat\", \"purrs\", \"when\", \"happy\", \"and\", \"content\"],\n",
        "    [\"animals\", \"need\", \"care\", \"and\", \"attention\"]\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Train initial model\n",
        "# -------------------------\n",
        "print(\"Training initial Word2Vec model...\")\n",
        "initial_model = Word2Vec(\n",
        "    sentences=initial_sentences,\n",
        "    vector_size=20,   # smaller size for tiny dataset\n",
        "    window=5,         # larger context window\n",
        "    min_count=1,\n",
        "    workers=1,\n",
        "    epochs=200        # more training passes\n",
        ")\n",
        "\n",
        "print(\"Most similar words to 'dog':\")\n",
        "try:\n",
        "    dog_similar = initial_model.wv.most_similar('dog', topn=3)\n",
        "    for word, score in dog_similar:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"  'dog' not found in vocabulary\")\n",
        "\n",
        "print(\"\\nMost similar words to 'cat':\")\n",
        "try:\n",
        "    cat_similar = initial_model.wv.most_similar('cat', topn=3)\n",
        "    for word, score in cat_similar:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"  'cat' not found in vocabulary\")\n",
        "\n",
        "# -------------------------\n",
        "# Additional sentences with puppy/kitten (extra reinforcement)\n",
        "# -------------------------\n",
        "additional_sentences = [\n",
        "    [\"puppy\", \"is\", \"a\", \"young\", \"dog\"],\n",
        "    [\"kitten\", \"is\", \"a\", \"baby\", \"cat\"],\n",
        "    [\"puppy\", \"loves\", \"to\", \"play\", \"and\", \"learn\"],\n",
        "    [\"kitten\", \"enjoys\", \"milk\", \"and\", \"soft\", \"toys\"],\n",
        "    [\"pet\", \"puppy\", \"brings\", \"happiness\", \"home\"],\n",
        "    [\"animal\", \"kitten\", \"is\", \"very\", \"cute\"],\n",
        "    [\"dog\", \"and\", \"puppy\", \"are\", \"best\", \"friends\"],\n",
        "    [\"cat\", \"teaches\", \"kitten\", \"to\", \"hunt\"],\n",
        "\n",
        "    # Reinforcement sentences for stronger similarity\n",
        "    [\"a\", \"puppy\", \"grows\", \"into\", \"a\", \"dog\"],\n",
        "    [\"a\", \"kitten\", \"grows\", \"into\", \"a\", \"cat\"],\n",
        "    [\"puppy\", \"and\", \"dog\", \"play\", \"together\"],\n",
        "    [\"kitten\", \"and\", \"cat\", \"sleep\", \"together\"]\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Incremental training on top of initial model\n",
        "# -------------------------\n",
        "print(\"\\nUpdating model with puppy/kitten sentences...\")\n",
        "initial_model.build_vocab(additional_sentences, update=True)\n",
        "initial_model.train(additional_sentences, total_examples=len(additional_sentences), epochs=200)\n",
        "\n",
        "# -------------------------\n",
        "# Results after expansion\n",
        "# -------------------------\n",
        "print(\"\\nAfter adding puppy/kitten sentences:\")\n",
        "print(\"Most similar words to 'dog':\")\n",
        "try:\n",
        "    dog_similar_new = initial_model.wv.most_similar('dog', topn=3)\n",
        "    for word, score in dog_similar_new:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"  'dog' not found in vocabulary\")\n",
        "\n",
        "print(\"\\nMost similar words to 'cat':\")\n",
        "try:\n",
        "    cat_similar_new = initial_model.wv.most_similar('cat', topn=3)\n",
        "    for word, score in cat_similar_new:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"  'cat' not found in vocabulary\")\n",
        "\n",
        "# -------------------------\n",
        "# Check puppy-dog and kitten-cat similarity\n",
        "# -------------------------\n",
        "try:\n",
        "    puppy_dog_sim = initial_model.wv.similarity('dog', 'puppy')\n",
        "    kitten_cat_sim = initial_model.wv.similarity('cat', 'kitten')\n",
        "    print(f\"\\nSimilarity between 'dog' and 'puppy': {puppy_dog_sim:.3f}\")\n",
        "    print(f\"Similarity between 'cat' and 'kitten': {kitten_cat_sim:.3f}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error calculating similarity: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjoAOSdJ-fl6",
        "outputId": "11dfa403-104e-438d-85f1-331bd4174513"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 4: Word2Vec Model Training (Improved Version)\n",
            "============================================================\n",
            "Training initial Word2Vec model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'dog':\n",
            "  happy: 0.577\n",
            "  when: 0.515\n",
            "  attention: 0.512\n",
            "\n",
            "Most similar words to 'cat':\n",
            "  care: 0.508\n",
            "  strangers: 0.368\n",
            "  play: 0.328\n",
            "\n",
            "Updating model with puppy/kitten sentences...\n",
            "\n",
            "After adding puppy/kitten sentences:\n",
            "Most similar words to 'dog':\n",
            "  brings: 0.800\n",
            "  is: 0.772\n",
            "  puppy: 0.770\n",
            "\n",
            "Most similar words to 'cat':\n",
            "  brings: 0.811\n",
            "  learn: 0.725\n",
            "  pet: 0.718\n",
            "\n",
            "Similarity between 'dog' and 'puppy': 0.770\n",
            "Similarity between 'cat' and 'kitten': 0.669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 5: TEXT NORMALIZATION CHALLENGE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 5: Text Normalization Challenge\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Install nltk for stemming/lemmatization (uncomment if needed)\n",
        "# !pip install nltk\n",
        "import nltk\n",
        "try:\n",
        "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "    from nltk.corpus import wordnet\n",
        "    # Download required NLTK data (uncomment if running for first time)\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk_available = True\n",
        "except:\n",
        "    nltk_available = False\n",
        "    print(\"NLTK not available, using basic normalization\")\n",
        "\n",
        "# Messy sentences with typos, punctuation, and mixed casing\n",
        "messy_sentences = [\n",
        "    \"ThIs iS a VeRy MeSSy SeNTeNcE!!! with lots of PuNcTuAtIoN???\",\n",
        "    \"programming   IS    really  INTERESTING,,,, but can be challenging...\",\n",
        "    \"I LOvE   data science & machine learning!!!\",\n",
        "    \"WHY are there SO many  SPACEs   and CAPS????\"\n",
        "]\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Comprehensive text cleaning function\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra punctuation (keep only letters, numbers, spaces)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Strip leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Simple tokenization\"\"\"\n",
        "    return text.split()\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Apply stemming to words\"\"\"\n",
        "    if nltk_available:\n",
        "        stemmer = PorterStemmer()\n",
        "        return [stemmer.stem(word) for word in words]\n",
        "    else:\n",
        "        # Simple suffix removal as fallback\n",
        "        stemmed = []\n",
        "        for word in words:\n",
        "            if word.endswith('ing'):\n",
        "                stemmed.append(word[:-3])\n",
        "            elif word.endswith('ed'):\n",
        "                stemmed.append(word[:-2])\n",
        "            elif word.endswith('s') and len(word) > 3:\n",
        "                stemmed.append(word[:-1])\n",
        "            else:\n",
        "                stemmed.append(word)\n",
        "        return stemmed\n",
        "\n",
        "def normalize_text_pipeline(text):\n",
        "    \"\"\"Complete text normalization pipeline\"\"\"\n",
        "    print(f\"Original: {text}\")\n",
        "\n",
        "    # Step 1: Clean text\n",
        "    cleaned = clean_text(text)\n",
        "    print(f\"Cleaned: {cleaned}\")\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    tokens = tokenize_text(cleaned)\n",
        "    print(f\"Tokenized: {tokens}\")\n",
        "\n",
        "    # Step 3: Stem\n",
        "    stemmed = stem_words(tokens)\n",
        "    print(f\"Stemmed: {stemmed}\")\n",
        "\n",
        "    # Step 4: Reconstruct\n",
        "    normalized = ' '.join(stemmed)\n",
        "    print(f\"Final: {normalized}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    return normalized\n",
        "\n",
        "print(\"Text Normalization Results:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "normalized_texts = []\n",
        "for sentence in messy_sentences:\n",
        "    normalized = normalize_text_pipeline(sentence)\n",
        "    normalized_texts.append(normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC0M5aIH-3Kz",
        "outputId": "9f77c66a-2aa1-4728-efd8-6282f91dd8fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 5: Text Normalization Challenge\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Normalization Results:\n",
            "============================================================\n",
            "Original: ThIs iS a VeRy MeSSy SeNTeNcE!!! with lots of PuNcTuAtIoN???\n",
            "Cleaned: this is a very messy sentence with lots of punctuation\n",
            "Tokenized: ['this', 'is', 'a', 'very', 'messy', 'sentence', 'with', 'lots', 'of', 'punctuation']\n",
            "Stemmed: ['thi', 'is', 'a', 'veri', 'messi', 'sentenc', 'with', 'lot', 'of', 'punctuat']\n",
            "Final: thi is a veri messi sentenc with lot of punctuat\n",
            "------------------------------------------------------------\n",
            "Original: programming   IS    really  INTERESTING,,,, but can be challenging...\n",
            "Cleaned: programming is really interesting but can be challenging\n",
            "Tokenized: ['programming', 'is', 'really', 'interesting', 'but', 'can', 'be', 'challenging']\n",
            "Stemmed: ['program', 'is', 'realli', 'interest', 'but', 'can', 'be', 'challeng']\n",
            "Final: program is realli interest but can be challeng\n",
            "------------------------------------------------------------\n",
            "Original: I LOvE   data science & machine learning!!!\n",
            "Cleaned: i love data science machine learning\n",
            "Tokenized: ['i', 'love', 'data', 'science', 'machine', 'learning']\n",
            "Stemmed: ['i', 'love', 'data', 'scienc', 'machin', 'learn']\n",
            "Final: i love data scienc machin learn\n",
            "------------------------------------------------------------\n",
            "Original: WHY are there SO many  SPACEs   and CAPS????\n",
            "Cleaned: why are there so many spaces and caps\n",
            "Tokenized: ['why', 'are', 'there', 'so', 'many', 'spaces', 'and', 'caps']\n",
            "Stemmed: ['whi', 'are', 'there', 'so', 'mani', 'space', 'and', 'cap']\n",
            "Final: whi are there so mani space and cap\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 6: FAKE NEWS HEADLINE DETECTOR (LOGISTIC REGRESSION)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 6: Fake News Headline Detector\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dataset of headlines (10 true, 10 fake)\n",
        "true_headlines = [\n",
        "    \"Scientists discover new species of deep-sea fish\",\n",
        "    \"Stock market closes higher after economic report\",\n",
        "    \"New vaccine shows promising results in clinical trials\",\n",
        "    \"City council approves budget for infrastructure improvements\",\n",
        "    \"University researchers develop more efficient solar panels\",\n",
        "    \"International trade agreement signed between two countries\",\n",
        "    \"Weather service issues flood warning for coastal areas\",\n",
        "    \"Technology company announces quarterly earnings report\",\n",
        "    \"Archaeological team uncovers ancient artifacts in excavation\",\n",
        "    \"Government releases new guidelines for public health\"\n",
        "]\n",
        "\n",
        "fake_headlines = [\n",
        "    \"Aliens secretly control world governments reveals insider\",\n",
        "    \"Miracle cure for all diseases hidden by pharmaceutical companies\",\n",
        "    \"Time traveler from 2050 warns about upcoming disasters\",\n",
        "    \"Scientists prove earth is actually flat using new technology\",\n",
        "    \"Celebrities are actually reptilian shapeshifters says expert\",\n",
        "    \"Government plans to replace all birds with surveillance drones\",\n",
        "    \"Ancient pyramid discovered to be giant alien spaceship\",\n",
        "    \"Drinking water from specific location grants immortality\",\n",
        "    \"Secret society controls all world events through mind control\",\n",
        "    \"Billionaire admits to being vampire in leaked recording\"\n",
        "]\n",
        "\n",
        "# Prepare training data for fake news detection\n",
        "X_news = true_headlines + fake_headlines\n",
        "y_news = [1] * len(true_headlines) + [0] * len(fake_headlines)  # 1 = true, 0 = fake\n",
        "\n",
        "# Vectorize headlines using TF-IDF\n",
        "news_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=100)\n",
        "X_news_vec = news_vectorizer.fit_transform(X_news)\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
        "    X_news_vec, y_news, test_size=0.3, random_state=42, stratify=y_news\n",
        ")\n",
        "\n",
        "# Train Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression(random_state=42)\n",
        "lr_classifier.fit(X_train_news, y_train_news)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_news = lr_classifier.predict(X_test_news)\n",
        "accuracy = accuracy_score(y_test_news, y_pred_news)\n",
        "\n",
        "print(f\"Fake News Detector Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Test on new headlines\n",
        "new_headlines = [\n",
        "    \"Researchers develop new method for cancer treatment\",\n",
        "    \"Unicorns found living in government secret facility\",\n",
        "    \"Company releases updated software with security improvements\",\n",
        "    \"Moon landing was staged on Hollywood movie set\"\n",
        "]\n",
        "\n",
        "X_new_vec = news_vectorizer.transform(new_headlines)\n",
        "new_predictions = lr_classifier.predict(X_new_vec)\n",
        "new_probabilities = lr_classifier.predict_proba(X_new_vec)\n",
        "\n",
        "print(\"\\nPredictions for new headlines:\")\n",
        "for i, headline in enumerate(new_headlines):\n",
        "    label = \"TRUE\" if new_predictions[i] == 1 else \"FAKE\"\n",
        "    confidence = max(new_probabilities[i]) * 100\n",
        "    print(f\"'{headline[:50]}...' → {label} (confidence: {confidence:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSO6zx89CBPD",
        "outputId": "6c7ce369-510f-4174-cfd8-947b3e16a80a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 6: Fake News Headline Detector\n",
            "==================================================\n",
            "Fake News Detector Accuracy: 0.33\n",
            "\n",
            "Predictions for new headlines:\n",
            "'Researchers develop new method for cancer treatmen...' → TRUE (confidence: 57.2%)\n",
            "'Unicorns found living in government secret facilit...' → FAKE (confidence: 55.5%)\n",
            "'Company releases updated software with security im...' → TRUE (confidence: 55.1%)\n",
            "'Moon landing was staged on Hollywood movie set...' → TRUE (confidence: 50.1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 7: BUILD A TINY SEARCH ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 7: Tiny Search Engine with TF-IDF\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Small document collection\n",
        "documents = [\n",
        "    \"Python is a powerful programming language for data science\",\n",
        "    \"Machine learning algorithms can solve complex problems\",\n",
        "    \"Data visualization helps understand patterns in datasets\",\n",
        "    \"Natural language processing analyzes text and speech\",\n",
        "    \"Deep learning models require large amounts of training data\",\n",
        "    \"Statistical analysis provides insights from numerical data\",\n",
        "    \"Computer vision processes and analyzes digital images\",\n",
        "    \"Big data technologies handle massive information volumes\"\n",
        "]\n",
        "\n",
        "class SimpleSearchEngine:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
        "\n",
        "    def search(self, query, top_k=3):\n",
        "        \"\"\"Search for most relevant documents\"\"\"\n",
        "        # Transform query using the same vectorizer\n",
        "        query_vec = self.vectorizer.transform([query])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
        "\n",
        "        # Get top k most similar documents\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:  # Only return relevant results\n",
        "                results.append({\n",
        "                    'document': self.documents[idx],\n",
        "                    'score': similarities[idx],\n",
        "                    'index': idx\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "# Create search engine\n",
        "search_engine = SimpleSearchEngine(documents)\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"machine learning data\",\n",
        "    \"image processing vision\",\n",
        "    \"python programming\",\n",
        "    \"statistical analysis\"\n",
        "]\n",
        "\n",
        "print(\"Search Engine Results:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"Query: '{query}'\")\n",
        "    results = search_engine.search(query)\n",
        "\n",
        "    if results:\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"  {i+1}. Score: {result['score']:.3f}\")\n",
        "            print(f\"     Document: {result['document']}\")\n",
        "    else:\n",
        "        print(\"  No relevant documents found\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1ZW3lVACWLp",
        "outputId": "1605e3c7-3f81-46df-d998-85abbd13bb07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 7: Tiny Search Engine with TF-IDF\n",
            "==================================================\n",
            "Search Engine Results:\n",
            "============================================================\n",
            "Query: 'machine learning data'\n",
            "  1. Score: 0.502\n",
            "     Document: Machine learning algorithms can solve complex problems\n",
            "  2. Score: 0.270\n",
            "     Document: Deep learning models require large amounts of training data\n",
            "  3. Score: 0.099\n",
            "     Document: Python is a powerful programming language for data science\n",
            "----------------------------------------\n",
            "Query: 'image processing vision'\n",
            "  1. Score: 0.304\n",
            "     Document: Natural language processing analyzes text and speech\n",
            "  2. Score: 0.296\n",
            "     Document: Computer vision processes and analyzes digital images\n",
            "----------------------------------------\n",
            "Query: 'python programming'\n",
            "  1. Score: 0.631\n",
            "     Document: Python is a powerful programming language for data science\n",
            "----------------------------------------\n",
            "Query: 'statistical analysis'\n",
            "  1. Score: 0.613\n",
            "     Document: Statistical analysis provides insights from numerical data\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 8: EMOJI PREDICTOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 8: Emoji Predictor\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Training data with sentences and corresponding emojis\n",
        "emoji_training_data = [\n",
        "    (\"I am very happy today\", \"😊\"),\n",
        "    (\"This is so exciting and amazing\", \"🎉\"),\n",
        "    (\"I love this wonderful day\", \"❤️\"),\n",
        "    (\"That was really scary\", \"😱\"),\n",
        "    (\"I am so angry about this\", \"😠\"),\n",
        "    (\"This makes me very sad\", \"😢\"),\n",
        "    (\"I am feeling tired\", \"😴\"),\n",
        "    (\"This is absolutely hilarious\", \"😂\"),\n",
        "    (\"I am surprised by this news\", \"😮\"),\n",
        "    (\"This food looks delicious\", \"🤤\"),\n",
        "    (\"I am confused about this\", \"😕\"),\n",
        "    (\"This weather is perfect\", \"☀️\"),\n",
        "    (\"I am worried about tomorrow\", \"😟\"),\n",
        "    (\"This party was fantastic\", \"🎊\"),\n",
        "    (\"I feel proud of my achievement\", \"😌\"),\n",
        "    (\"This situation is frustrating\", \"😤\"),\n",
        "    (\"I am grateful for everything\", \"🙏\"),\n",
        "    (\"This movie was boring\", \"😑\"),\n",
        "    (\"I am excited for vacation\", \"✈️\"),\n",
        "    (\"This music is great\", \"🎵\")\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "emoji_sentences = [item[0] for item in emoji_training_data]\n",
        "emoji_labels = [item[1] for item in emoji_training_data]\n",
        "\n",
        "# Create label encoder for emojis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "emoji_encoder = LabelEncoder()\n",
        "encoded_labels = emoji_encoder.fit_transform(emoji_labels)\n",
        "\n",
        "# Vectorize sentences\n",
        "emoji_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "X_emoji = emoji_vectorizer.fit_transform(emoji_sentences)\n",
        "\n",
        "# Train classifier\n",
        "emoji_classifier = MultinomialNB()\n",
        "emoji_classifier.fit(X_emoji, encoded_labels)\n",
        "\n",
        "# Test on new sentences\n",
        "test_sentences_emoji = [\n",
        "    \"I feel extremely joyful\",\n",
        "    \"This situation terrifies me\",\n",
        "    \"I am very sleepy right now\",\n",
        "    \"This joke is so funny\",\n",
        "    \"The weather is beautiful today\"\n",
        "]\n",
        "\n",
        "X_test_emoji = emoji_vectorizer.transform(test_sentences_emoji)\n",
        "emoji_predictions = emoji_classifier.predict(X_test_emoji)\n",
        "predicted_emojis = emoji_encoder.inverse_transform(emoji_predictions)\n",
        "\n",
        "print(\"Emoji Prediction Results:\")\n",
        "for i, sentence in enumerate(test_sentences_emoji):\n",
        "    print(f\"'{sentence}' → {predicted_emojis[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAKzc76CCrPe",
        "outputId": "0e963861-c497-45e4-e488-9498ace26308"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 8: Emoji Predictor\n",
            "==================================================\n",
            "Emoji Prediction Results:\n",
            "'I feel extremely joyful' → 😌\n",
            "'This situation terrifies me' → 😤\n",
            "'I am very sleepy right now' → ☀️\n",
            "'This joke is so funny' → ☀️\n",
            "'The weather is beautiful today' → ☀️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 9: AUTHOR STYLE DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 9: Author Style Detection\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Shakespeare samples (simplified/modernized for demo)\n",
        "shakespeare_samples = [\n",
        "    \"to be or not to be that is the question\",\n",
        "    \"all the world is a stage and all men and women merely players\",\n",
        "    \"what light through yonder window breaks it is the east\",\n",
        "    \"fair is foul and foul is fair hover through fog and filthy air\",\n",
        "    \"now is the winter of our discontent made glorious summer\",\n",
        "    \"double double toil and trouble fire burn and cauldron bubble\",\n",
        "    \"out out brief candle life is but walking shadow\",\n",
        "    \"friends romans countrymen lend me your ears\",\n",
        "    \"cowards die many times before their deaths valiant never taste death\",\n",
        "    \"lord what fools these mortals be\"\n",
        "]\n",
        "\n",
        "# Jane Austen samples (simplified)\n",
        "austen_samples = [\n",
        "    \"it is truth universally acknowledged that single man in possession good fortune\",\n",
        "    \"pride relates more to our opinion of ourselves vanity to what we would have others think\",\n",
        "    \"happiness in marriage is entirely matter of chance\",\n",
        "    \"there is nothing like staying at home for real comfort\",\n",
        "    \"silly things do cease to be silly if they are done by sensible people\",\n",
        "    \"one half of the world cannot understand pleasures of the other\",\n",
        "    \"woman especially if she have misfortune of knowing anything should conceal it\",\n",
        "    \"friendship is certainly finest balm for pangs of disappointed love\",\n",
        "    \"nothing is more deceitful than appearance of humility\",\n",
        "    \"angry people are not always wise\"\n",
        "]\n",
        "\n",
        "# Prepare author classification data\n",
        "author_texts = shakespeare_samples + austen_samples\n",
        "author_labels = [0] * len(shakespeare_samples) + [1] * len(austen_samples)  # 0 = Shakespeare, 1 = Austen\n",
        "\n",
        "# Vectorize the texts\n",
        "author_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=100)\n",
        "X_authors = author_vectorizer.fit_transform(author_texts)\n",
        "\n",
        "# Split data\n",
        "X_train_authors, X_test_authors, y_train_authors, y_test_authors = train_test_split(\n",
        "    X_authors, author_labels, test_size=0.3, random_state=42, stratify=author_labels\n",
        ")\n",
        "\n",
        "# Train classifier\n",
        "author_classifier = LogisticRegression(random_state=42)\n",
        "author_classifier.fit(X_train_authors, y_train_authors)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_authors = author_classifier.predict(X_test_authors)\n",
        "author_accuracy = accuracy_score(y_test_authors, y_pred_authors)\n",
        "\n",
        "print(f\"Author Classification Accuracy: {author_accuracy:.2f}\")\n",
        "\n",
        "# Test on new texts\n",
        "new_texts = [\n",
        "    \"something is rotten in state of denmark\",  # Shakespeare style\n",
        "    \"young lady of seventeen cannot be really serious yet\",  # Austen style\n",
        "    \"brevity is soul of wit\",  # Shakespeare\n",
        "    \"nothing ever fatigues me but doing what I do not like\"  # Austen\n",
        "]\n",
        "\n",
        "X_new_authors = author_vectorizer.transform(new_texts)\n",
        "author_predictions = author_classifier.predict(X_new_authors)\n",
        "author_probabilities = author_classifier.predict_proba(X_new_authors)\n",
        "\n",
        "print(\"\\nAuthor Prediction Results:\")\n",
        "authors = [\"Shakespeare\", \"Austen\"]\n",
        "for i, text in enumerate(new_texts):\n",
        "    predicted_author = authors[author_predictions[i]]\n",
        "    confidence = max(author_probabilities[i]) * 100\n",
        "    print(f\"'{text}' → {predicted_author} (confidence: {confidence:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guSKWGSvC2Fo",
        "outputId": "57e547c8-586e-4df6-e67b-0b77d86b92a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 9: Author Style Detection\n",
            "==================================================\n",
            "Author Classification Accuracy: 0.67\n",
            "\n",
            "Author Prediction Results:\n",
            "'something is rotten in state of denmark' → Shakespeare (confidence: 50.0%)\n",
            "'young lady of seventeen cannot be really serious yet' → Shakespeare (confidence: 50.0%)\n",
            "'brevity is soul of wit' → Shakespeare (confidence: 50.0%)\n",
            "'nothing ever fatigues me but doing what I do not like' → Austen (confidence: 54.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TASK 10: CREATIVE GENERATION WITH N-GRAM MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\nTASK 10: Creative Text Generation with N-gram Model\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Fairy tale text corpus\n",
        "fairy_tale_text = \"\"\"\n",
        "once upon a time there was a beautiful princess who lived in a tall tower.\n",
        "the princess had long golden hair that sparkled in the sunlight.\n",
        "every day the princess would look out of her window hoping for rescue.\n",
        "one day a brave knight came riding through the forest on his white horse.\n",
        "the knight saw the tower and heard the princess singing sweetly.\n",
        "he climbed up the tower using the princess long golden hair.\n",
        "when they met they fell in love immediately and lived happily ever after.\n",
        "the end of this magical fairy tale story.\n",
        "\"\"\"\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, text, n=2):\n",
        "        self.n = n\n",
        "        self.ngrams = defaultdict(list)\n",
        "        self.build_model(text)\n",
        "\n",
        "    def build_model(self, text):\n",
        "        \"\"\"Build n-gram model from text\"\"\"\n",
        "        # Clean and tokenize\n",
        "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        words = text.split()\n",
        "\n",
        "        # Create n-grams\n",
        "        for i in range(len(words) - self.n + 1):\n",
        "            prefix = tuple(words[i:i + self.n - 1])\n",
        "            next_word = words[i + self.n - 1]\n",
        "            self.ngrams[prefix].append(next_word)\n",
        "\n",
        "    def generate_text(self, seed_words, length=20):\n",
        "        \"\"\"Generate text using n-gram model\"\"\"\n",
        "        import random\n",
        "\n",
        "        # Start with seed words\n",
        "        if isinstance(seed_words, str):\n",
        "            current = tuple(seed_words.lower().split())\n",
        "        else:\n",
        "            current = tuple(seed_words)\n",
        "\n",
        "        generated = list(current)\n",
        "\n",
        "        for _ in range(length):\n",
        "            if current in self.ngrams:\n",
        "                # Choose next word randomly from possibilities\n",
        "                next_word = random.choice(self.ngrams[current])\n",
        "                generated.append(next_word)\n",
        "\n",
        "                # Update current context\n",
        "                current = current[1:] + (next_word,)\n",
        "            else:\n",
        "                # If no continuation found, restart with random n-gram\n",
        "                if self.ngrams:\n",
        "                    current = random.choice(list(self.ngrams.keys()))\n",
        "                    generated.append(current[0])\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        return ' '.join(generated)\n",
        "\n",
        "# Create bigram model (n=2)\n",
        "bigram_model = NGramModel(fairy_tale_text, n=2)\n",
        "\n",
        "print(\"Generated Fairy Tale Stories:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Generate multiple stories with different seeds\n",
        "seeds = [\n",
        "    \"once upon\",\n",
        "    \"the princess\",\n",
        "    \"a brave\",\n",
        "    \"they lived\"\n",
        "]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    story = bigram_model.generate_text(seed, length=15)\n",
        "    print(f\"Generated: {story}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Create trigram model (n=3) for comparison\n",
        "print(\"\\nTrigram Model Results:\")\n",
        "trigram_model = NGramModel(fairy_tale_text, n=3)\n",
        "\n",
        "for seed in [\"once upon a\", \"the princess had\"]:\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    story = trigram_model.generate_text(seed, length=12)\n",
        "    print(f\"Generated: {story}\")\n",
        "    print(\"-\" * 40)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyq11HIhDPHr",
        "outputId": "fcdc12fa-5d2c-4123-ed27-f94eac71864b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TASK 10: Creative Text Generation with N-gram Model\n",
            "==================================================\n",
            "Generated Fairy Tale Stories:\n",
            "========================================\n",
            "Seed: 'once upon'\n",
            "Generated: once upon sunlight every day a tall tower the princess had long golden hair when they met\n",
            "----------------------------------------\n",
            "Seed: 'the princess'\n",
            "Generated: the princess rescue one day a brave knight saw the forest on his white horse the end\n",
            "----------------------------------------\n",
            "Seed: 'a brave'\n",
            "Generated: a brave knight came riding through the forest on his white horse the forest on his white\n",
            "----------------------------------------\n",
            "Seed: 'they lived'\n",
            "Generated: they lived he climbed up the princess singing sweetly he climbed up the forest on his white\n",
            "----------------------------------------\n",
            "\n",
            "Trigram Model Results:\n",
            "Seed: 'once upon a'\n",
            "Generated: once upon a tower heard the princess long golden hair when they met they fell\n",
            "----------------------------------------\n",
            "Seed: 'the princess had'\n",
            "Generated: the princess had a princess who lived in a tall tower the princess singing sweetly\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Complete NLP Tasks - Summary Comments\n",
        "Task 1: Custom Stopword Removal\n",
        "Purpose: Remove common words that don't add semantic value\n",
        "Key Learning: Custom stopwords vs standard lists, impact on text processing\n",
        "Output: Shows word count reduction and cleaner text for analysis\n",
        "Task 2: Vocabulary Builder - Term-Document Matrix\n",
        "Purpose: Create numerical representation of text documents\n",
        "Key Learning: How words are distributed across documents, sparse matrix concepts\n",
        "Output: DataFrame showing word frequencies per document, vocabulary statistics\n",
        "Task 3: Mini Sentiment Classifier (Naive Bayes)\n",
        "Purpose: Classify text as positive or negative sentiment\n",
        "Key Learning: Supervised learning, probabilistic classification\n",
        "Output: Accuracy scores and predictions on new sentences with confidence levels\n",
        "Task 4: Word2Vec Model Training\n",
        "Purpose: Learn word embeddings and semantic relationships\n",
        "Key Learning: How adding related words improves semantic similarity\n",
        "Output: Similar words to \"dog\"/\"cat\" before/after adding \"puppy\"/\"kitten\" data\n",
        "Task 5: Text Normalization Challenge\n",
        "Purpose: Clean messy text data for processing\n",
        "Key Learning: Text preprocessing pipeline - cleaning, tokenizing, stemming\n",
        "Output: Step-by-step transformation from messy to clean text\n",
        "Task 6: Fake News Headline Detector\n",
        "Purpose: Classify headlines as real or fake news\n",
        "Key Learning: Binary classification, TF-IDF features, logistic regression\n",
        "Output: Model accuracy and predictions on new headlines with confidence\n",
        "Task 7: Tiny Search Engine\n",
        "Purpose: Build document retrieval system using similarity\n",
        "Key Learning: TF-IDF vectorization, cosine similarity for ranking\n",
        "Output: Ranked relevant documents for different search queries\n",
        "Task 8: Emoji Predictor\n",
        "Purpose: Predict appropriate emoji for given text\n",
        "Key Learning: Multi-class classification, text-to-symbol mapping\n",
        "Output: Emoji predictions for emotional expressions in sentences\n",
        "Task 9: Author Style Detection\n",
        "Purpose: Identify writing style differences between authors\n",
        "Key Learning: Stylistic analysis, feature extraction from writing patterns\n",
        "Output: Classification of text samples as Shakespeare vs Jane Austen style\n",
        "Task 10: Creative Text Generation\n",
        "Purpose: Generate new text in similar style using statistical patterns\n",
        "Key Learning: N-gram models, probability-based text generation\n",
        "Output: Generated fairy tale sentences using bigram/trigram patterns\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "vUaOJi4yDjIh",
        "outputId": "6227e611-cd19-43ce-8765-8c727ad0596e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Complete NLP Tasks - Summary Comments\\nTask 1: Custom Stopword Removal\\nPurpose: Remove common words that don\\'t add semantic value\\nKey Learning: Custom stopwords vs standard lists, impact on text processing\\nOutput: Shows word count reduction and cleaner text for analysis\\nTask 2: Vocabulary Builder - Term-Document Matrix\\nPurpose: Create numerical representation of text documents\\nKey Learning: How words are distributed across documents, sparse matrix concepts\\nOutput: DataFrame showing word frequencies per document, vocabulary statistics\\nTask 3: Mini Sentiment Classifier (Naive Bayes)\\nPurpose: Classify text as positive or negative sentiment\\nKey Learning: Supervised learning, probabilistic classification\\nOutput: Accuracy scores and predictions on new sentences with confidence levels\\nTask 4: Word2Vec Model Training\\nPurpose: Learn word embeddings and semantic relationships\\nKey Learning: How adding related words improves semantic similarity\\nOutput: Similar words to \"dog\"/\"cat\" before/after adding \"puppy\"/\"kitten\" data\\nTask 5: Text Normalization Challenge\\nPurpose: Clean messy text data for processing\\nKey Learning: Text preprocessing pipeline - cleaning, tokenizing, stemming\\nOutput: Step-by-step transformation from messy to clean text\\nTask 6: Fake News Headline Detector\\nPurpose: Classify headlines as real or fake news\\nKey Learning: Binary classification, TF-IDF features, logistic regression\\nOutput: Model accuracy and predictions on new headlines with confidence\\nTask 7: Tiny Search Engine\\nPurpose: Build document retrieval system using similarity\\nKey Learning: TF-IDF vectorization, cosine similarity for ranking\\nOutput: Ranked relevant documents for different search queries\\nTask 8: Emoji Predictor\\nPurpose: Predict appropriate emoji for given text\\nKey Learning: Multi-class classification, text-to-symbol mapping\\nOutput: Emoji predictions for emotional expressions in sentences\\nTask 9: Author Style Detection\\nPurpose: Identify writing style differences between authors\\nKey Learning: Stylistic analysis, feature extraction from writing patterns\\nOutput: Classification of text samples as Shakespeare vs Jane Austen style\\nTask 10: Creative Text Generation\\nPurpose: Generate new text in similar style using statistical patterns\\nKey Learning: N-gram models, probability-based text generation\\nOutput: Generated fairy tale sentences using bigram/trigram patterns'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmRGBrCjEIEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}