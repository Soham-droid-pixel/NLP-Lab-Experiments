{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo2oU751GWnF",
        "outputId": "676b10b1-ad17-4a67-a2d4-843b07640514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soham\n"
          ]
        }
      ],
      "source": [
        "print(\"Soham\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy rapidfuzz nltk scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEj5JaR8Gi0I",
        "outputId": "9837f75a-a6fa-4426-e67c-1cb273bd951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQCZnaDLJx_N",
        "outputId": "6f46c02a-b277-4292-aea3-6043c2c98803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from rapidfuzz.distance import Levenshtein\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download NLTK tokenizer\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaWKr0qhJ4Pv",
        "outputId": "20b3a980-2cb1-49da-8b18-bd77b3bb5407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_TEXT = (\n",
        "    \"Hey @john_doe, check out https://example.com/event on 15-08-2025!\\n\"\n",
        "    \"Call me at +1-202-555-0183 or email jane.doe1990@mail.org ASAP.\\n\"\n",
        "    \"This product is damn awful!! #fail #WorstDayEver\\n\"\n",
        "    \"Big news: OpenAI opens new office in Paris. üöÄ\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "dtey5Ac1KLE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InfoExtractor:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    def extract_urls(self):\n",
        "        return re.findall(r'https?://\\S+', self.text)\n",
        "\n",
        "    def extract_dates(self):\n",
        "        return re.findall(r'\\b\\d{2}-\\d{2}-\\d{4}\\b', self.text)\n",
        "\n",
        "    def extract_emails(self):\n",
        "        return re.findall(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', self.text)\n",
        "\n",
        "    def extract_phone_numbers(self):\n",
        "        return re.findall(r'\\+?\\d{1,3}-\\d{3}-\\d{3}-\\d{4}', self.text)\n",
        "\n",
        "    def extract_mentions(self):\n",
        "        return re.findall(r'@\\w+', self.text)\n",
        "\n",
        "    def extract_hashtags(self):\n",
        "        return re.findall(r'#\\w+', self.text)\n",
        "\n",
        "    def extract_offensive_words(self):\n",
        "        offensive_list = ['damn', 'awful']  # Add more as needed\n",
        "        return [word for word in self.text.lower().split() if word.strip('!.,') in offensive_list]\n"
      ],
      "metadata": {
        "id": "zEfHkriPKW9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = InfoExtractor(SAMPLE_TEXT)\n",
        "print(\"‚úÖ URLs:\", extractor.extract_urls())\n",
        "print(\"‚úÖ Dates:\", extractor.extract_dates())\n",
        "print(\"‚úÖ Emails:\", extractor.extract_emails())\n",
        "print(\"‚úÖ Phone Numbers:\", extractor.extract_phone_numbers())\n",
        "print(\"‚úÖ Mentions:\", extractor.extract_mentions())\n",
        "print(\"‚úÖ Hashtags:\", extractor.extract_hashtags())\n",
        "print(\"‚úÖ Offensive Words:\", extractor.extract_offensive_words())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY1LslcwKlpL",
        "outputId": "5c00e800-4bd1-4a72-a697-b5c2193c2911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ URLs: ['https://example.com/event']\n",
            "‚úÖ Dates: ['15-08-2025']\n",
            "‚úÖ Emails: ['jane.doe1990@mail.org']\n",
            "‚úÖ Phone Numbers: ['+1-202-555-0183']\n",
            "‚úÖ Mentions: ['@john_doe', '@mail']\n",
            "‚úÖ Hashtags: ['#fail', '#WorstDayEver']\n",
            "‚úÖ Offensive Words: ['damn', 'awful!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NgramModel:\n",
        "    def __init__(self, text, n):\n",
        "        self.n = n\n",
        "        self.tokens = nltk.word_tokenize(text.lower())\n",
        "        self.ngrams = list(nltk.ngrams(self.tokens, n))\n",
        "        self.freq_dist = Counter(self.ngrams)\n",
        "\n",
        "    def get_ngram_count(self, ngram):\n",
        "        return self.freq_dist[ngram]\n",
        "\n",
        "    def get_mle_probability(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_count = sum(1 for ng in self.ngrams if ng[:-1] == prefix)\n",
        "        return self.freq_dist[ngram] / prefix_count if prefix_count > 0 else 0\n"
      ],
      "metadata": {
        "id": "v0M1YMwlKqd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "uni = NgramModel(SAMPLE_TEXT, 1)\n",
        "bi = NgramModel(SAMPLE_TEXT, 2)\n",
        "tri = NgramModel(SAMPLE_TEXT, 3)\n",
        "\n",
        "print(\"Unigram count of ('the',):\", uni.get_ngram_count(('the',)))\n",
        "print(\"Bigram count of ('check', 'out'):\", bi.get_ngram_count(('check', 'out')))\n",
        "print(\"Trigram MLE P(('check', 'out', 'https')):\", tri.get_mle_probability(('check', 'out', 'https')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwWDupyqK2z7",
        "outputId": "912ef130-c2d1-4334-d76d-cc73db1e6636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram count of ('the',): 0\n",
            "Bigram count of ('check', 'out'): 1\n",
            "Trigram MLE P(('check', 'out', 'https')): 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_levenshtein(str1, str2):\n",
        "    return Levenshtein.distance(str1, str2)\n",
        "\n",
        "# üß™ Example\n",
        "post1 = \"Hey @john_doe, check out the event!\"\n",
        "post2 = \"Hey @john_doe, check the event!\"\n",
        "\n",
        "print(\"üîÅ Levenshtein Distance:\", compute_levenshtein(post1, post2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnP5LAk0K50w",
        "outputId": "0ef3b655-467e-420b-b7f8-1277ab2ae2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Levenshtein Distance: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinguisticAnalyzer:\n",
        "    def __init__(self, text):\n",
        "        self.doc = nlp(text)\n",
        "\n",
        "    def get_noun_chunks(self):\n",
        "        return [chunk.text for chunk in self.doc.noun_chunks]\n",
        "\n",
        "    def get_verbs(self):\n",
        "        return [token.text for token in self.doc if token.pos_ == \"VERB\"]\n",
        "\n",
        "    def get_named_entities(self):\n",
        "        return [(ent.text, ent.label_) for ent in self.doc.ents]\n"
      ],
      "metadata": {
        "id": "P5uenWR8Lr8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = LinguisticAnalyzer(SAMPLE_TEXT)\n",
        "\n",
        "print(\"üìå Noun Chunks:\", analyzer.get_noun_chunks())\n",
        "print(\"üìå Verbs:\", analyzer.get_verbs())\n",
        "print(\"üìå Named Entities (NER):\", analyzer.get_named_entities())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MGExAX8L1oC",
        "outputId": "cd39a346-b422-4371-9ecc-01989879ad50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Noun Chunks: ['me', '+1-202-555-0183 or email jane.doe1990@mail.org ASAP', 'This product', '#WorstDayEver\\nBig news', 'OpenAI', 'new office', 'Paris']\n",
            "üìå Verbs: ['check', 'Call', 'email', 'fail', 'opens']\n",
            "üìå Named Entities (NER): [('15-08-2025', 'DATE'), ('jane.doe1990@mail.org ASAP', 'PERSON'), ('#fail #', 'MONEY'), ('Paris', 'GPE'), ('üöÄ', 'CARDINAL')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Summary ===\")\n",
        "print(\"‚úÖ URLs:\", extractor.extract_urls())\n",
        "print(\"‚úÖ Emails:\", extractor.extract_emails())\n",
        "print(\"‚úÖ Phone Numbers:\", extractor.extract_phone_numbers())\n",
        "print(\"‚úÖ Hashtags:\", extractor.extract_hashtags())\n",
        "print(\"‚úÖ NER:\", analyzer.get_named_entities())\n",
        "print(\"‚úÖ Offensive:\", extractor.extract_offensive_words())\n",
        "print(\"‚úÖ Levenshtein ('event', 'event!'):\", compute_levenshtein(\"event\", \"event!\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBg7r-R5L5Ix",
        "outputId": "ea7f6d5f-8506-4a9c-9bc9-ada5cbd5638a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Summary ===\n",
            "‚úÖ URLs: ['https://example.com/event']\n",
            "‚úÖ Emails: ['jane.doe1990@mail.org']\n",
            "‚úÖ Phone Numbers: ['+1-202-555-0183']\n",
            "‚úÖ Hashtags: ['#fail', '#WorstDayEver']\n",
            "‚úÖ NER: [('15-08-2025', 'DATE'), ('jane.doe1990@mail.org ASAP', 'PERSON'), ('#fail #', 'MONEY'), ('Paris', 'GPE'), ('üöÄ', 'CARDINAL')]\n",
            "‚úÖ Offensive: ['damn', 'awful!!']\n",
            "‚úÖ Levenshtein ('event', 'event!'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKhpKUrrL90N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}