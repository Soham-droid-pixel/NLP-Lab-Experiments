{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "4FepFfGGbLcl",
        "outputId": "17bad6e6-967e-45e0-a7d6-547971a13cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "387ff6147bec48259ca3677eec4b2d70"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install nltk gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Dataset\n",
        "docs = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog barked at the mailman\",\n",
        "    \"Cats and dogs are popular pets\",\n",
        "    \"The mat was sat on by the cat\",\n",
        "    \"Dogs bark loudly at strangers\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USBAD594b_XO",
        "outputId": "f3947c5a-cae2-4646-e9c0-812502ca8394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt_tab\") # Added to fix the error\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Dataset\n",
        "docs = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog barked at the mailman\",\n",
        "    \"Cats and dogs are popular pets\",\n",
        "    \"The mat was sat on by the cat\",\n",
        "    \"Dogs bark loudly at strangers\"\n",
        "]\n",
        "\n",
        "# -------------------\n",
        "# Task 1: Tokenize\n",
        "# -------------------\n",
        "tokenized_docs = [word_tokenize(doc) for doc in docs]\n",
        "print(\"Tokenized:\", tokenized_docs)\n",
        "\n",
        "# -------------------\n",
        "# Task 2: Lowercase\n",
        "# -------------------\n",
        "lower_docs = [[w.lower() for w in doc] for doc in tokenized_docs]\n",
        "print(\"Lowercased:\", lower_docs)\n",
        "\n",
        "# -------------------\n",
        "# Task 3: Remove Stopwords\n",
        "# -------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "no_stop_docs = [[w for w in doc if w not in stop_words] for doc in lower_docs]\n",
        "print(\"No Stopwords:\", no_stop_docs)\n",
        "\n",
        "# -------------------\n",
        "# Task 4: Stemming\n",
        "# -------------------\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_docs = [[stemmer.stem(w) for w in doc] for doc in no_stop_docs]\n",
        "print(\"Stemmed:\", stemmed_docs)\n",
        "\n",
        "# -------------------\n",
        "# Task 5: Lemmatization\n",
        "# -------------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_docs = [[lemmatizer.lemmatize(w) for w in doc] for doc in no_stop_docs]\n",
        "print(\"Lemmatized:\", lemmatized_docs)\n",
        "\n",
        "# -------------------\n",
        "# Task 6: Vocabulary\n",
        "# -------------------\n",
        "vocab = sorted(set([w for doc in lemmatized_docs for w in doc]))\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
        "\n",
        "#Task 8:TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
        "\n",
        "#Task9:Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
        "\n",
        "# Train Word2Vec\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=3, min_count=1, sg=0)\n",
        "\n",
        "print(\"Vector for 'cat':\\n\", w2v_model.wv['cat'])\n",
        "print(\"Most similar to 'dog':\\n\", w2v_model.wv.most_similar('dog'))\n",
        "\n",
        "#Task 10:Preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess (lowercase + remove stopwords + lemmatize)\n",
        "processed_docs = []\n",
        "for doc in docs:\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in doc.split() if word.lower() not in stop_words]\n",
        "    processed_docs.append(\" \".join(tokens))\n",
        "\n",
        "print(\"Processed Docs:\", processed_docs)\n",
        "\n",
        "# Vectorize with TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "print(\"Final Vocabulary:\", tfidf.get_feature_names_out())\n",
        "print(\"Final TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl9VHN_4ccoG",
        "outputId": "cd21a326-f5ac-40af-dc23-516f52320ff3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized: [['The', 'cat', 'sat', 'on', 'the', 'mat'], ['The', 'dog', 'barked', 'at', 'the', 'mailman'], ['Cats', 'and', 'dogs', 'are', 'popular', 'pets'], ['The', 'mat', 'was', 'sat', 'on', 'by', 'the', 'cat'], ['Dogs', 'bark', 'loudly', 'at', 'strangers']]\n",
            "Lowercased: [['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'barked', 'at', 'the', 'mailman'], ['cats', 'and', 'dogs', 'are', 'popular', 'pets'], ['the', 'mat', 'was', 'sat', 'on', 'by', 'the', 'cat'], ['dogs', 'bark', 'loudly', 'at', 'strangers']]\n",
            "No Stopwords: [['cat', 'sat', 'mat'], ['dog', 'barked', 'mailman'], ['cats', 'dogs', 'popular', 'pets'], ['mat', 'sat', 'cat'], ['dogs', 'bark', 'loudly', 'strangers']]\n",
            "Stemmed: [['cat', 'sat', 'mat'], ['dog', 'bark', 'mailman'], ['cat', 'dog', 'popular', 'pet'], ['mat', 'sat', 'cat'], ['dog', 'bark', 'loudli', 'stranger']]\n",
            "Lemmatized: [['cat', 'sat', 'mat'], ['dog', 'barked', 'mailman'], ['cat', 'dog', 'popular', 'pet'], ['mat', 'sat', 'cat'], ['dog', 'bark', 'loudly', 'stranger']]\n",
            "Vocabulary: ['bark', 'barked', 'cat', 'dog', 'loudly', 'mailman', 'mat', 'pet', 'popular', 'sat', 'stranger']\n",
            "Vocabulary: ['and' 'are' 'at' 'bark' 'barked' 'by' 'cat' 'cats' 'dog' 'dogs' 'loudly'\n",
            " 'mailman' 'mat' 'on' 'pets' 'popular' 'sat' 'strangers' 'the' 'was']\n",
            "BoW Matrix:\n",
            " [[0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 2 0]\n",
            " [0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 2 0]\n",
            " [1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 2 1]\n",
            " [0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0]]\n",
            "Vocabulary: ['and' 'are' 'at' 'bark' 'barked' 'by' 'cat' 'cats' 'dog' 'dogs' 'loudly'\n",
            " 'mailman' 'mat' 'on' 'pets' 'popular' 'sat' 'strangers' 'the' 'was']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "  0.38472354 0.         0.         0.         0.         0.\n",
            "  0.38472354 0.38472354 0.         0.         0.38472354 0.\n",
            "  0.63871058 0.        ]\n",
            " [0.         0.         0.34575201 0.         0.42855071 0.\n",
            "  0.         0.         0.42855071 0.         0.         0.42855071\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.57401081 0.        ]\n",
            " [0.42066906 0.42066906 0.         0.         0.         0.\n",
            "  0.         0.42066906 0.         0.33939315 0.         0.\n",
            "  0.         0.         0.42066906 0.42066906 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.39535516\n",
            "  0.31897006 0.         0.         0.         0.         0.\n",
            "  0.31897006 0.31897006 0.         0.         0.31897006 0.\n",
            "  0.52954792 0.39535516]\n",
            " [0.         0.         0.38898761 0.48214012 0.         0.\n",
            "  0.         0.         0.         0.38898761 0.48214012 0.\n",
            "  0.         0.         0.         0.         0.         0.48214012\n",
            "  0.         0.        ]]\n",
            "Vector for 'cat':\n",
            " [-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
            "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
            " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
            " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
            "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
            " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
            "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
            "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
            "  0.00180291  0.01278507]\n",
            "Most similar to 'dog':\n",
            " [('strangers', 0.2373521625995636), ('mailman', 0.1845553070306778), ('pets', 0.17922039330005646), ('sat', 0.13940522074699402), ('mat', 0.10702362656593323), ('by', 0.0507713258266449), ('loudly', 0.045559048652648926), ('on', -0.01015650574117899), ('popular', -0.028643256053328514), ('the', -0.05607093498110771)]\n",
            "Processed Docs: ['cat sat mat', 'dog barked mailman', 'cat dog popular pet', 'mat sat cat', 'dog bark loudly stranger']\n",
            "Final Vocabulary: ['bark' 'barked' 'cat' 'dog' 'loudly' 'mailman' 'mat' 'pet' 'popular'\n",
            " 'sat' 'stranger']\n",
            "Final TF-IDF Matrix:\n",
            " [[0.         0.         0.50620441 0.         0.         0.\n",
            "  0.60981846 0.         0.         0.60981846 0.        ]\n",
            " [0.         0.63907044 0.         0.42799292 0.         0.63907044\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.39346994 0.39346994 0.         0.\n",
            "  0.         0.58752141 0.58752141 0.         0.        ]\n",
            " [0.         0.         0.50620441 0.         0.         0.\n",
            "  0.60981846 0.         0.         0.60981846 0.        ]\n",
            " [0.53849791 0.         0.         0.36063833 0.53849791 0.\n",
            "  0.         0.         0.         0.         0.53849791]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 – Tokenize Each Sentence\n",
        "# Tokenization is the process of splitting text into smaller units called tokens (words).\n",
        "# Example: \"The cat sat on the mat\" → [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "# This step is essential for almost all NLP preprocessing tasks.\n",
        "\n",
        "# Task 2 – Lowercase All Sentences\n",
        "# Converting all text to lowercase ensures uniformity.\n",
        "# Example: \"The Cat\" and \"the cat\" will be treated as the same word.\n",
        "# This reduces redundancy in vocabulary.\n",
        "\n",
        "# Task 3 – Remove Stopwords\n",
        "# Stopwords are common words like \"the\", \"is\", \"on\", etc., which usually don’t add much meaning.\n",
        "# Removing them reduces noise and focuses on meaningful words.\n",
        "\n",
        "# Task 4 – Apply Stemming\n",
        "# Stemming reduces words to their root form by chopping off suffixes.\n",
        "# Example: \"running\", \"runs\", \"ran\" → \"run\"\n",
        "# It is rule-based and sometimes produces non-dictionary words.\n",
        "\n",
        "# Task 5 – Apply Lemmatization\n",
        "# Lemmatization also reduces words to their root form but uses vocabulary and grammar rules.\n",
        "# Example: \"better\" → \"good\", \"running\" → \"run\"\n",
        "# More accurate than stemming, produces real dictionary words.\n",
        "\n",
        "# Task 6 – Build Vocabulary\n",
        "# Vocabulary is the set of unique words across all documents after preprocessing.\n",
        "# Example: [\"cat\", \"dog\", \"mat\", \"bark\", \"mailman\", ...]\n",
        "# Building vocabulary is the basis for creating word vectors.\n",
        "\n",
        "# Task 7 – Create a BoW Term-Document Matrix\n",
        "# Bag of Words (BoW) represents text as word counts in a matrix.\n",
        "# Rows → Documents, Columns → Words from vocabulary.\n",
        "# Each cell shows how many times a word appears in a document.\n",
        "# Example:\n",
        "#   \"The cat sat on the mat\" → [cat:1, dog:0, mat:1, sat:1, ...]\n",
        "\n",
        "# Task 8 – Compute Simple TF-IDF\n",
        "# TF-IDF (Term Frequency – Inverse Document Frequency) assigns weight to words.\n",
        "# TF = frequency of word in a document / total words in the document.\n",
        "# IDF = log(total documents / documents containing the word).\n",
        "# This reduces the importance of common words and highlights rare but important ones.\n",
        "\n",
        "# Task 9 – Train a Small Word2Vec Model\n",
        "# Word2Vec learns vector representations of words (dense embeddings).\n",
        "# Words with similar context will have similar vectors.\n",
        "# Example: \"dog\" and \"puppy\" may have similar embeddings.\n",
        "# Useful for semantic similarity, clustering, and deep NLP tasks.\n",
        "\n",
        "# Task 10 – Preprocess & Vectorize\n",
        "# This combines all preprocessing steps: tokenization, lowercasing, stopword removal,\n",
        "# stemming/lemmatization, and vocabulary building.\n",
        "# Then, text is converted into numerical vectors (BoW, TF-IDF, or Word2Vec).\n",
        "# These vectors are used as input features for machine learning or deep learning models.\n"
      ],
      "metadata": {
        "id": "pSZaFluWclv3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ak-Dzv9g-7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}